# π“ 2025/09/20
- **WSL2 + Ubuntu μ„¤μΉ λ° ν™κ²½ μ„Έν…**
- **Docker Desktop + NVIDIA GPU μ—°λ™**
- **vLLM μ„λ²„ μ‹¤ν–‰ μ„±κ³µ**
  - `TinyLlama/TinyLlama-1.1B-Chat-v1.0` λ¨λΈ λ΅λ“
  - API μ„λ²„ μ •μƒ κµ¬λ™ (`http://localhost:8000/v1`)
- **ν…μ¤νΈ μ½”λ“ μ‘μ„± (`test_llm.py`)**
  - OpenAI API νΈν™ λ°©μ‹μΌλ΅ λ΅μ»¬ LLM νΈμ¶ μ„±κ³µ
  - κ°„λ‹¨ν• λ€ν™” μ‘λ‹µ ν™•μΈ μ™„λ£
- **λ³΄μ• κ΄€λ¦¬**
  - Hugging Face Access Tokenμ΄ λ…Έμ¶λλ” λ¬Έμ λ¥Ό λ°κ²¬
  - `.gitignore`μ— `.env` μ¶”κ°€ν•΄μ„ ν•΄κ²°

---

## π“‚ ν΄λ” & νμΌ κµ¬μ΅°

T_project/
β”β”€β”€ ai/
β”‚ β””β”€β”€ llama-runtime/
β”‚ β”β”€β”€ docker-compose.yml # vLLM μ„λ²„ μ‹¤ν–‰ μ„¤μ •
β”‚ β”β”€β”€ .env # Hugging Face Token λ“± ν™κ²½ λ³€μ (Gitμ— κ³µμ  X)
β”‚ β”β”€β”€ test_llm.py # λ΅μ»¬ vLLM μ„λ²„ ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”‚ β””β”€β”€ requirements.txt # Python μμ΅΄μ„± (μµμ…)
β”β”€β”€ README.md # ν”„λ΅μ νΈ μ„¤λ… λ¬Έμ„
β””β”€β”€ .gitignore # Gitμ— μ¬λ¦¬μ§€ μ•μ„ νμΌ λ©λ΅ (.env ν¬ν•¨)

---

## π“ κ° νμΌ μ—­ν• 

### **docker-compose.yml**
- vLLM μ„λ²„ μ‹¤ν–‰ μ •μ
- μ‚¬μ©ν•  λ¨λΈ, ν¬νΈ, GPU λ©”λ¨λ¦¬ ν™μ© λΉ„μ¨ λ“± μ„¤μ •

### **.env**
- Hugging Face Access Token μ €μ¥
- **κ³µμ  κΈμ§€** (λ³΄μ• μ΄μ λ•λ¬Έμ— `.gitignore`μ— μ¶”κ°€λ¨)

### **test_llm.py**
- Pythonμ—μ„ vLLM(OpenAI API νΈν™) νΈμ¶ μμ 
- λ΅μ»¬ μ„λ²„ ν…μ¤νΈ μ©λ„

### **requirements.txt**
- ν•„μ”ν• Python ν¨ν‚¤μ§€ λ…μ‹ (μ: `openai`, `requests`)

### **.gitignore**
- Gitμ— μ¬λ¦¬μ§€ μ•μ•„μ•Ό ν•λ” νμΌ μ μ™Έ  
  - `.env`  
  - `__pycache__/`  
  - `*.pyc`  
