# ============================================================
# vLLM 서버 실행 설정 파일
# ------------------------------------------------------------
# 실행 방법:
#   1) 해당 폴더로 이동:
#        cd ai/llama-runtime
#   2) 백그라운드 실행:
#        docker compose up -d --build
#   3) 로그 확인:
#        docker compose logs -f
#   4) 서버 중지:
#        docker compose down
# ============================================================

version: "3.8"

services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"   # 로컬 8000 포트로 API 접근
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}   # Hugging Face 토큰 사용
    command: >
      --host 0.0.0.0
      --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
      --max-model-len 2048
      --gpu-memory-utilization 0.75
