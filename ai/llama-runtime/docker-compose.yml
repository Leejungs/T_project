# ============================================================
# vLLM 서버 실행 설정 파일
# ------------------------------------------------------------
# 실행 방법:
#   1) 해당 폴더로 이동:
#        cd ai/llama-runtime
#   2) 백그라운드 실행:
#        docker compose up -d --build
#   3) 로그 확인:
#        docker compose logs -f
#   4) 서버 중지:
#        docker compose down
# ============================================================

services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
      VLLM_LOGGING_LEVEL: DEBUG          # 문제 시 원인 파악 도움
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    command: >
      --host 0.0.0.0
      --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
      --max-model-len 2048
      --gpu-memory-utilization 0.75